{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "## examples of sentiment analysis, topic analysis, and text generation\n",
    "\n",
    "1. Define question - What makes one comedian's material different from another's?\n",
    "2. Get and clean data\n",
    "3. Perform exploratory data analysis\n",
    "4. Apply analsyis techniques\n",
    "    + sentiment analysis\n",
    "    +  topic analysis\n",
    "    + text generation\n",
    "5. Share findings\n",
    "\n",
    "# 1. Define question - What makes one comedian's material different from another's?\n",
    "\n",
    "Use methods like sentiment analysis to see how comedians differ by thier material."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "import spacy\n",
    "import requests\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data collection\n",
    "\n",
    "    1. Define the scope of the data to be used (what and how much)\n",
    "    2. Define where you can get this data\n",
    "    3. Have a plane for storage\n",
    "\n",
    "## 2.1 Gather data by way of web scraping - Beautiful soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def url_to_transcript(url):\n",
    "#     '''\n",
    "#     Returns transcriipt data from the target website, scrapsfromtheloft.com.\n",
    "#     Content is taken from the \"post-content\" class.\n",
    "#     '''\n",
    "#     page = requests.get(url).text\n",
    "#     soup = BeautifulSoup(page, 'lxml')\n",
    "#     text = [p.text for p in soup.find(class_=\"post-content\").find_all('p')]\n",
    "#     print(url)\n",
    "#     return text\n",
    "\n",
    "\n",
    "# # URLs of transcripts in scope\n",
    "# urls = [\n",
    "#     'http://scrapsfromtheloft.com/2017/05/06/louis-ck-oh-my-god-full-transcript/',\n",
    "#     'http://scrapsfromtheloft.com/2017/04/11/dave-chappelle-age-spin-2017-full-transcript/',\n",
    "#     'http://scrapsfromtheloft.com/2018/03/15/ricky-gervais-humanity-transcript/',\n",
    "#     'http://scrapsfromtheloft.com/2017/08/07/bo-burnham-2013-full-transcript/',\n",
    "#     'http://scrapsfromtheloft.com/2017/05/24/bill-burr-im-sorry-feel-way-2014-full-transcript/',\n",
    "#     'http://scrapsfromtheloft.com/2017/04/21/jim-jefferies-bare-2014-full-transcript/',\n",
    "#     'http://scrapsfromtheloft.com/2017/08/02/john-mulaney-comeback-kid-2015-full-transcript/',\n",
    "#     'http://scrapsfromtheloft.com/2017/10/21/hasan-minhaj-homecoming-king-2017-full-transcript/',\n",
    "#     'http://scrapsfromtheloft.com/2017/09/19/ali-wong-baby-cobra-2016-full-transcript/',\n",
    "#     'http://scrapsfromtheloft.com/2017/08/03/anthony-jeselnik-thoughts-prayers-2015-full-transcript/',\n",
    "#     'http://scrapsfromtheloft.com/2018/03/03/mike-birbiglia-my-girlfriends-boyfriend-2013-full-transcript/',\n",
    "#     'http://scrapsfromtheloft.com/2017/08/19/joe-rogan-triggered-2016-full-transcript/'\n",
    "# ]\n",
    "\n",
    "# # Use comedian's short names, in order with the URLs listed, as keys to their respective content.\n",
    "# comedians = ['louis', 'dave', 'ricky', 'bo', 'bill', 'jim',\n",
    "#              'john', 'hasan', 'ali', 'anthony', 'mike', 'joe']\n",
    "\n",
    "# # request transcripts (takes a few minutes to run)\n",
    "# transcripts = [url_to_transcript(url) for url in urls]\n",
    "\n",
    "# # Save the work as a pickled file for use later\n",
    "# !mkdir transcripts\n",
    "# for index, comedian in enumerate(comedians):\n",
    "#     with open('transcripts/' + comedian + '.txt', 'wb') as file:\n",
    "#         pickle.dump(transcripts[index], file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Clean the data\n",
    "\n",
    "1. Get the corpus i.e collect all the data into a table with the comedian on one cloumn and their material in the next column, in a dataframe (Pandas).\n",
    "2. Create a document-term matrix:\n",
    "    + clean text - remove unneccessary parts of text, punctuation, etc.\n",
    "    + tokenize the text - change the 'words' into machine usable symbols\n",
    "    + create document matrix - put the document into a form the machine can understand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comedians = ['louis', 'dave', 'ricky', 'bo', 'bill', 'jim',\n",
    "             'john', 'hasan', 'ali', 'anthony', 'mike', 'joe']\n",
    "# Load pickled files and check that the data has been recovered\n",
    "data = {} # use a dictionary with the comedians' names as keys and the transcripts as values\n",
    "for i, comedian in enumerate(comedians):\n",
    "    with open('transcripts/' + comedian + '.txt', 'rb') as file:\n",
    "        data[comedian] = pickle.load(file)\n",
    "# check key, names\n",
    "# print('{}\\n'.format(data.keys()))\n",
    "\n",
    "# # check value, text. You might notice that there are non-ascii values in the corpus\n",
    "# print('louis:\\t{}\\n'.format(data['louis'][:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When dealing with numerical data, data cleaning often involves removing null values and duplicate data, dealing with outliers, etc. With text data, there are some common data cleaning techniques, which are also known as text pre-processing techniques.\n",
    "\n",
    "With text data, this cleaning process can go on forever. There are always exceptions to every cleaning step. So, we're going to follow the MVP (minimum viable product) approach - start simple and iterate. Here are a bunch of things you can do to clean your data. We're going to execute just the common cleaning steps here and the rest can be done at a later point to improve the results.\n",
    "\n",
    "Common data cleaning steps on all text:\n",
    "\n",
    "* Make text all lower case\n",
    "* Remove punctuation\n",
    "* Remove numerical values\n",
    "* Remove common non-sensical text (e.g. /n)\n",
    "* Tokenize text (break a block of text into sentences or words)\n",
    "* Remove stop words\n",
    "\n",
    "More data cleaning steps after tokenization:\n",
    "* Stemming / Lemmatization\n",
    "* Parts of speech tagging\n",
    "* Create bi-grams or tri-grams\n",
    "* Deal with typos and misspelled words\n",
    "Do these later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(data.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that our dictionary is currently in key: comedian, value: list of text format\n",
    "next(iter(data.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to change this to key: comedian, value: string format\n",
    "def combine_text(list_of_text):\n",
    "    '''\n",
    "    Takes a list of text and combines them into one large chunk of text.\n",
    "    '''\n",
    "    # Combine all lines of a comedian's material into one line of text.\n",
    "    combined_text = ' '.join(list_of_text)\n",
    "    return combined_text\n",
    "\n",
    "# create a data object, a dictionary, with the name as key and the combined text as the value.\n",
    "data_combined = {key: [combine_text(value)] for (key, value) in data.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the data into a dataframe for easier data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Set the width of the display to show a limited muber of characters, to avoid word-wrap\n",
    "pd.set_option('max_colwidth',125)\n",
    "\n",
    "# flip the data by its leaning diagonal, so a comedian's name is in the first column and the text is in the second\n",
    "data_df = pd.DataFrame.from_dict(data_combined).transpose()\n",
    "data_df.columns = ['transcript']\n",
    "data_df = data_df.sort_index()\n",
    "data_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's take a look at the transcript for Ali Wong\n",
    "data_df.transcript.loc['ali']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Tokenize the data - sentence, fragment, word\n",
    "\n",
    "### Apply a first round of text cleaning techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "# data_df.head()\n",
    "\n",
    "def clean_text(text):\n",
    "    ''' \n",
    "    Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers. \n",
    "    '''\n",
    "    text = text.lower() # make text lower case\n",
    "    text = re.sub('\\\\[.*?\\\\]', '', text) # remove any brackets and the content within it\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text) # remove the punctuation marks (string.punctuation)\n",
    "    text = re.sub('\\\\w*\\\\d\\\\w*', '', text) # remove an words with numbers in them\n",
    "    text = re.sub('[‘’“”…]', '', text) # remove non-ascii quotes and apostrophies\n",
    "    text = re.sub('\\\\n', '', text) # remove line break characters\n",
    "    text = re.sub('\\s{2,}', ' ', text) # remove multiple spaces\n",
    "    return text\n",
    "    \n",
    "#==============================================================================\n",
    "# Make use of the apply function. Identify the series in the dataframe, \n",
    "# followed by the function you want to apply to that series, and add the name \n",
    "# of the function, without the parenteses within the parentheses of the apply \n",
    "# method. THis will return the results of the referenced method to each element \n",
    "# in the series. In this case data_df.transcript is the series submitted and for\n",
    "# each element in that series, it is replaced by the result of the function.\n",
    "data_df.transcript = data_df.transcript.apply(clean_text)\n",
    "\n",
    "# Let's take a look at the updated text\n",
    "data_df.transcript\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# Let's add the comedians' full names as well\n",
    "full_names = ['Ali Wong', 'Anthony Jeselnik', 'Bill Burr', 'Bo Burnham', \n",
    "    'Dave Chappelle', 'Hasan Minhaj','Jim Jefferies', 'Joe Rogan', \n",
    "    'John Mulaney', 'Louis C.K.', 'Mike Birbiglia', 'Ricky Gervais']\n",
    "\n",
    "# add the full names of the comedians\n",
    "data_df['full_name'] = full_names\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# Let's pickle it for later use\n",
    "data_df.to_pickle('data_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# Load the dictionary back from the pickle file.\n",
    "import pickle\n",
    "\n",
    "data_df = pickle.load( open( \"data_df.pkl\", \"rb\" ) )\n",
    "# data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document-Term Matrix\n",
    "\n",
    "For many of the techniques we'll be using in future notebooks, the text must be \n",
    "tokenized, meaning broken down into smaller pieces. The most common tokenization \n",
    "technique is to break down text into words. We can do this using scikit-learn's \n",
    "CountVectorizer, where every row will represent a different document and every \n",
    "column will represent a different word.\n",
    "\n",
    "In addition, with CountVectorizer, we can remove stop words. Stop words are common \n",
    "words that add no additional meaning to text such as 'a', 'the', etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# define the instance of CountVectorizer and add a reference to the stop words\n",
    "# in English. We will add a list of special stop-words later.\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Process the trascripts, removing the stop words and put it into a matrix.\n",
    "data_cv = cv.fit_transform(data_df.transcript)\n",
    "\n",
    "#==============================================================================\n",
    "# Save this spot for Lemitizing and Stemming words...\n",
    "#==============================================================================\n",
    "\n",
    "# convert data object into an array\n",
    "data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_dtm.index = data_clean.index\n",
    "data_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dtm.to_pickle(\"./data_dtm.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Exploratory Data Analysis\n",
    "\n",
    "Use the corpus and the document-term matrix to perform EDA to figure out the main trends in the data and see if it makes sense.\n",
    "\n",
    "* Data: determine the format of the raw data you will need to begin analysis\n",
    "* Aggregate: Figure out how to aggregate the data\n",
    "* Visualize: find the best way to visualize the data \n",
    "* Insights: Extract some key insights from the visualizations\n",
    "\n",
    "Use frequency, or word counts, to see what topic or subject seems to keep being mentioned.\n",
    "\n",
    "* top word(s)\n",
    "* vocabulary\n",
    "* jargon or specialty words (differentiating it from other texts)\n",
    "\n",
    "### Top words run a tally for top words for each comedian. You can visualize the text as a word cloud, bar plot, scatter plot, etc. to see what one comedian looks like when compared to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "#  retrieve the pickled file\n",
    "data = pd.read_pickle('data_dtm.pkl')\n",
    "#==============================================================================\n",
    "# We will now transpose the datafram on the leaning diagonal so we can tally \n",
    "# the words used by each comedian. This is easier to do when the values are in \n",
    "# a column.\n",
    "data = data.transpose()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the top 30 words said by each comedian\n",
    "top_dict = {}\n",
    "for comedian in data.columns:\n",
    "    top = data[comedian].sort_values(ascending=False).head(30)\n",
    "    top_dict[comedian]= list(zip(top.index, top.values))\n",
    "\n",
    "top_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the top 15 words said by each comedian\n",
    "for comedian, top_words in top_dict.items():\n",
    "    print(comedian)\n",
    "    print(', '.join([word for word, count in top_words[0:14]]))\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: At this point, we could go on and create word clouds. However, by looking\n",
    "at these top words, you can see that some of them have very little meaning and \n",
    "could be added to a stop words list, so let's do just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# Look at the most common top words --> add them to the stop word list\n",
    "from collections import Counter\n",
    "\n",
    "# Let's first pull out the top 30 words for each comedian\n",
    "words = []\n",
    "for comedian in data.columns:\n",
    "    top = [word for (word, count) in top_dict[comedian]]\n",
    "    for t in top:\n",
    "        words.append(t)\n",
    "# words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's aggregate this list and identify the most common words along with how many routines they occur in\n",
    "# Counter(words).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If more than half of the comedians have it as a top word, exclude it from the list\n",
    "add_stop_words = [word for word, count in Counter(words).most_common() if count > 6]\n",
    "add_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's update our document-term matrix with the new list of stop words\n",
    "from sklearn.feature_extraction import text \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Read in cleaned data\n",
    "data_clean = pd.read_pickle('data_clean.pkl')\n",
    "\n",
    "# Add new stop words\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
    "\n",
    "# Recreate document-term matrix\n",
    "cv = CountVectorizer(stop_words=stop_words)\n",
    "data_cv = cv.fit_transform(data_clean.transcript)\n",
    "data_stop = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_stop.index = data_clean.index\n",
    "\n",
    "# Pickle it for later use\n",
    "import pickle\n",
    "pickle.dump(cv, open(\"cv_stop.pkl\", \"wb\"))\n",
    "data_stop.to_pickle(\"dtm_stop.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make some word clouds!\n",
    "# Terminal / Anaconda Prompt: conda install -c conda-forge wordcloud\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# wc = WordCloud(stopwords=stop_words, background_color=\"white\", colormap=\"Dark2\", max_font_size=150, random_state=42)\n",
    "wc = WordCloud(stopwords=stop_words, background_color=\"black\", max_font_size=150, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the output dimensions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [16, 6]\n",
    "\n",
    "full_names = ['Ali Wong', 'Anthony Jeselnik', 'Bill Burr', 'Bo Burnham', 'Dave Chappelle', 'Hasan Minhaj',\n",
    "              'Jim Jefferies', 'Joe Rogan', 'John Mulaney', 'Louis C.K.', 'Mike Birbiglia', 'Ricky Gervais']\n",
    "\n",
    "# Create subplots for each comedian\n",
    "for index, comedian in enumerate(data.columns):\n",
    "    wc.generate(data_clean.transcript[comedian])\n",
    "    \n",
    "    plt.subplot(3, 4, index+1)\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(full_names[index])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put data into a martix for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "# Find the number of unique words that each comedian uses\n",
    "# # Identify the non-zero items in the document-term matrix, meaning that the word occurs at least once\n",
    "unique_list = []\n",
    "for comedian in data.columns:\n",
    "    uniques = data[comedian].to_numpy().nonzero()[0].size\n",
    "    unique_list.append(uniques)\n",
    "\n",
    "data_words = pd.DataFrame(list(zip(full_names, unique_list)), columns=['comedian', 'unique_words'])\n",
    "data_unique_sort = data_words.sort_values(by='unique_words')\n",
    "data_unique_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the words per minute of each comedian\n",
    "\n",
    "# Find the total number of words that a comedian uses\n",
    "total_list = []\n",
    "for comedian in data.columns:\n",
    "    totals = sum(data[comedian])\n",
    "    total_list.append(totals)\n",
    "    \n",
    "# Comedy special run times from IMDB, in minutes\n",
    "run_times = [60, 59, 80, 60, 67, 73, 77, 63, 62, 58, 76, 79]\n",
    "\n",
    "# Let's add some columns to our dataframe\n",
    "data_words['total_words'] = total_list\n",
    "data_words['run_times'] = run_times\n",
    "data_words['words_per_minute'] = data_words['total_words'] / data_words['run_times']\n",
    "\n",
    "# Sort the dataframe by words per minute to see who talks the slowest and fastest\n",
    "data_wpm_sort = data_words.sort_values(by='words_per_minute')\n",
    "data_wpm_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot our findings\n",
    "import numpy as np\n",
    "\n",
    "y_pos = np.arange(len(data_words))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.barh(y_pos, data_unique_sort.unique_words, align='center')\n",
    "plt.yticks(y_pos, data_unique_sort.comedian)\n",
    "plt.title('Number of Unique Words', fontsize=20)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.barh(y_pos, data_wpm_sort.words_per_minute, align='center')\n",
    "plt.yticks(y_pos, data_wpm_sort.comedian)\n",
    "plt.title('Number of Words Per Minute', fontsize=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Vocabulary**\n",
    "   * Ricky Gervais (British comedy) and Bill Burr (podcast host) use a lot of words in their comedy\n",
    "   * Louis C.K. (self-depricating comedy) and Anthony Jeselnik (dark humor) have a smaller vocabulary\n",
    "\n",
    "\n",
    "* **Talking Speed**\n",
    "   * Joe Rogan (blue comedy) and Bill Burr (podcast host) talk fast\n",
    "   * Bo Burnham (musical comedy) and Anthony Jeselnik (dark humor) talk slow\n",
    "   \n",
    "Ali Wong is somewhere in the middle in both cases. Nothing too interesting here.\n",
    "\n",
    "## Amount of Profanity\n",
    "\n",
    "Assume that profanity is a distraction; F- is used for every part of speech as a filler word. At the same time saying 'that was shit' has the opposite meaning to 'that was the shit' - idiomatic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Earlier I said we'd revisit profanity. Let's take a look at the most common words again.\n",
    "# Counter(words).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's isolate just these 'blue' words and see how frequntly they are used by a given comedian.\n",
    "data_blue_words = data.transpose()[['fucking', 'fuck', 'shit', 'nigga']]\n",
    "data_profanity = pd.concat([data_blue_words.fucking + data_blue_words.fuck, data_blue_words.shit, data_blue_words.nigga], axis=1)\n",
    "data_profanity.columns = ['f_word', 's_word', 'n_word']\n",
    "data_profanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a scatter plot of our findings\n",
    "plt.rcParams['figure.figsize'] = [10, 8]\n",
    "\n",
    "for i, comedian in enumerate(data_profanity.index):\n",
    "    x = data_profanity.f_word.loc[comedian]\n",
    "    y = data_profanity.s_word.loc[comedian]\n",
    "    plt.scatter(x, y, color='blue')\n",
    "    plt.text(x+1.5, y+0.5, full_names[i], fontsize=10)\n",
    "    plt.xlim(-5, 155) \n",
    "    \n",
    "plt.title('Number of Blue Words Used in Routine', fontsize=20)\n",
    "plt.xlabel('Number of F Bombs', fontsize=15)\n",
    "plt.ylabel('Number of S Words', fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a 3D plot of Blue words used by the comedians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import axes3d\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "# ax = plt.axes(projection='3d')\n",
    "\n",
    "# Data for a three-dimensional line\n",
    "for i, comedian in enumerate(data_profanity.index):\n",
    "    x = data_profanity.f_word.loc[comedian]\n",
    "    y = data_profanity.s_word.loc[comedian]\n",
    "    z = data_profanity.n_word.loc[comedian]\n",
    "    ax.scatter(x, y, z)\n",
    "\n",
    "ax.set_title('Blue Words')\n",
    "ax.set_xlabel('F Bombs')\n",
    "ax.set_ylabel('S Word')\n",
    "ax.set_zlabel('N Word')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Who talks about family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's isolate just the words that relate to family and see how frequntly they are used by a given comedian.\n",
    "data_family_words = data.transpose()[['dad', 'mom', 'kids', 'husband', 'wife','grandma']]\n",
    "data_family = pd.concat([data_family_words.dad, data_family_words.mom, data_family_words.kids, data_family_words.husband, data_family_words.wife, data_family_words.grandma], axis=1)\n",
    "data_family.columns = ['dad', 'mom', 'kids', 'husband', 'wife','grandma']\n",
    "data_family"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does that look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [10, 8]\n",
    "\n",
    "for i, comedian in enumerate(data_family.index):\n",
    "    x = data_family.dad.loc[comedian]\n",
    "    y = data_family.mom.loc[comedian]\n",
    "    plt.scatter(x, y, color='blue')\n",
    "    plt.text(x+1.5, y+0.5, full_names[i], fontsize=10)\n",
    "    plt.xlim(-5, 155) \n",
    "\n",
    "# plt.xlim(0,70)\n",
    "# plt.xlim(0,'auto')\n",
    "plt.set(xlim=(xmin, xmax), ylim=(ymin, ymax))\n",
    "plt.title('Number of References to Parents in a Routine', fontsize=20)\n",
    "plt.xlabel('Number of Dads', fontsize=15)\n",
    "plt.ylabel('Number of Moms', fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(5))\n",
    "end = 5\n",
    "plt.plot(range(end))\n",
    "\n",
    "# plt.xlim(-5, 5)\n",
    "plt.xlim(0, end)\n",
    "# plt.ylim(-5, 5)\n",
    "plt.ylim(0, end)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # <test>\n",
    "# import numpy  \n",
    "\n",
    "# # Set values of data points\n",
    "# # Note, this uses a numpy array which is not the same as a list\n",
    "\n",
    "# # n = 10\n",
    "# # x,y,z = np.random.normal(0,1,(3, n))\n",
    "# x = [.1,.2,.3,.4,.5,.6,.7,.8,.3,.4,.5,.6,.7,.8,.1,.2,.6,.7,.3,.8,.1,.2,.4,.5] # define a regular list\n",
    "# x = numpy.array(x) # make regular list into a numpy list\n",
    "\n",
    "# y = [.3,.4,.5,.6,.7,.8,.1,.2,.6,.7,.3,.8,.1,.2,.4,.5,.1,.2,.3,.4,.5,.6,.7,.8]\n",
    "# # y = [3,4,5,6,7,8,1,2,6,7,3,8,1,2,4,5,1,2,3,4,5,6,7,8]\n",
    "# y = numpy.array(y)\n",
    "\n",
    "# z = [.6,.7,.3,.8,.1,.2,.4,.5,.1,.2,.3,.4,.5,.6,.7,.8,.3,.4,.5,.6,.7,.8,.1,.2]\n",
    "# z = numpy.array(z)\n",
    "# # </test>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy \n",
    "import ipyvolume as ipv \n",
    "fig = ipv.figure()\n",
    "ipv.pylab.xlim(0, 1) # Set limits of x axis.\n",
    "ipv.pylab.ylim(0, 1) # Set limits of y axis.\n",
    "ipv.pylab.zlim(0, 1) # Set limits of z axis.\n",
    "\n",
    "ipv.scatter(x,y,z,marker='sphere')\n",
    "# scatter = ipv.scatter(x,y,z,marker='sphere') # scatter is an object you can use later\n",
    "ipv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's isolate just these 'blue' words and see how frequntly they are used by a given comedian.\n",
    "data_blue_words2 = data.transpose()[['fucking', 'fuck', 'shit', 'nigga']]\n",
    "data_profanity2 = pd.concat([data_blue_words.fucking + data_blue_words.fuck, data_blue_words.shit, data_blue_words.nigga], axis=1)\n",
    "data_profanity2.columns = ['f_word', 's_word', 'n_word']\n",
    "data_profanity2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipyvolume as ipv2 \n",
    "import numpy \n",
    "\n",
    "fig2 = ipv.figure()\n",
    "ipv.pylab.xlim(0, 120) # Set limits of x axis.\n",
    "ipv.pylab.ylim(0, 120) # Set limits of y axis.\n",
    "ipv.pylab.zlim(0, 120) # Set limits of z axis.\n",
    "\n",
    "f_ = s_ = n_ = []\n",
    "\n",
    "# Data for a three-dimensional line\n",
    "for i, comedian in enumerate(data_profanity2.index):\n",
    "    f = data_profanity2['f_word'][comedian]\n",
    "    s = data_profanity2['s_word'][comedian]\n",
    "    n = data_profanity2['n_word'][comedian]\n",
    "    print('{}: {}, {}, {}'.format(comedian,f,s,n))\n",
    "    f_.append(f)\n",
    "    s_.append(s)\n",
    "    n_.append(n)\n",
    "\n",
    "f_ = numpy.array(f_)\n",
    "s_ = numpy.array(s_)\n",
    "n_ = numpy.array(n_)\n",
    "\n",
    "ipv2.scatter(f_, s_, n_, marker='sphere')\n",
    "ipv2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "So far, all of the analysis we've done has been pretty generic - looking at counts, creating scatter plots, etc. These techniques could be applied to numeric data as well.\n",
    "\n",
    "When it comes to text data, there are a few popular techniques that we'll be going through starting with sentiment analysis. A few key points to remember with sentiment analysis.\n",
    "\n",
    "* TextBlob Module: Linguistic researchers have labeled the sentiment of words based on their domain expertise. Sentiment of words can vary based on where it is in a sentence. The TextBlob module allows us to take advantage of these labels.\n",
    "* Sentiment Labels: Each word in a corpus is labeled in terms of polarity and subjectivity (there are more labels as well, but we're going to ignore them for now). A corpus' sentiment is the average of these.\n",
    "* Polarity: How positive or negative a word is. -1 is very negative. +1 is very positive.\n",
    "* Subjectivity: How subjective, or opinionated a word is. 0 is fact. +1 is very much an opinion.\n",
    "For more info on how TextBlob coded up its [sentiment function](https://planspace.org/20150607-textblob_sentiment/).\n",
    "\n",
    "Let's take a look at the sentiment of the various transcripts, both overall and throughout the comedy routine.\n",
    "\n",
    "## Sentiment of Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll start by reading in the corpus, which preserves word order\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_pickle('corpus.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, a few words about textblob\n",
    "\n",
    "* https://github.com/sloria/TextBlob \n",
    "* for full documentation, go to https://textblob.readthedocs.io/.\n",
    "* Features\n",
    "* Noun phrase extraction\n",
    "* Part-of-speech tagging\n",
    "* Sentiment analysis\n",
    "* Classification (Naive Bayes, Decision Tree)\n",
    "* Tokenization (splitting text into words and sentences)\n",
    "* Word and phrase frequencies\n",
    "* Parsing\n",
    "* n-grams\n",
    "* Word inflection (pluralization and singularization) and lemmatization\n",
    "* Spelling correction\n",
    "* Add new models or languages through extensions\n",
    "* WordNet integration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create quick lambda functions to find the polarity and subjectivity of each routine\n",
    "# Terminal / Anaconda Navigator: conda install -c conda-forge textblob\n",
    "from textblob import TextBlob\n",
    "\n",
    "pol = lambda x: TextBlob(x).sentiment.polarity\n",
    "sub = lambda x: TextBlob(x).sentiment.subjectivity\n",
    "\n",
    "data['polarity'] = data['transcript'].apply(pol)\n",
    "data['subjectivity'] = data['transcript'].apply(sub)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 8]\n",
    "\n",
    "for index, comedian in enumerate(data.index):\n",
    "    x = data.polarity.loc[comedian]\n",
    "    y = data.subjectivity.loc[comedian]\n",
    "    plt.scatter(x, y, color='blue')\n",
    "    plt.text(x+.001, y+.001, data['full_name'][index], fontsize=10)\n",
    "    plt.xlim(-.01, .12) \n",
    "    \n",
    "plt.title('Sentiment Analysis', fontsize=20)\n",
    "plt.xlabel('<-- Negative -------- Positive -->', fontsize=15)\n",
    "plt.ylabel('<-- Facts -------- Opinions -->', fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reprint the graph to demonstrate differences in the full range of sentiment and objectivity. Now can comedians be thought of as a group?\n",
    "\n",
    "They appear to be a pretty tight cluster after all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the results - the bigger picture\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 8]\n",
    "\n",
    "for index, comedian in enumerate(data.index):\n",
    "    x = data.polarity.loc[comedian]\n",
    "    y = data.subjectivity.loc[comedian]\n",
    "    plt.scatter(x, y, color='blue')\n",
    "    plt.text(x+.001, y+.001, data['full_name'][index], fontsize=10)\n",
    "    plt.xlim(-1, 1) \n",
    "    plt.ylim(0, 1) \n",
    "    \n",
    "plt.title('Sentiment Analysis', fontsize=20)\n",
    "plt.xlabel('<-- Negative -------- Positive -->', fontsize=15)\n",
    "plt.ylabel('<-- Facts -------- Opinions -->', fontsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Longitudinal Sentiment Analysis\n",
    "Instead of looking at the overall sentiment, let's see if there's anything interesting about the sentiment over time throughout each routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split each routine into 10 parts\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def split_text(text, n=10):\n",
    "    '''Takes in a string of text and splits into n equal parts, with a default of 10 equal parts.'''\n",
    "\n",
    "    # Calculate length of text, the size of each chunk of text and the starting points of each chunk of text\n",
    "    length = len(text)\n",
    "    size = math.floor(length / n)\n",
    "    start = np.arange(0, length, size)\n",
    "    \n",
    "    # Pull out equally sized pieces of text and put it into a list\n",
    "    split_list = []\n",
    "    for piece in range(n):\n",
    "        split_list.append(text[start[piece]:start[piece]+size])\n",
    "    return split_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at our data again\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a list to hold all of the pieces of text\n",
    "list_pieces = []\n",
    "for t in data.transcript:\n",
    "    split = split_text(t)\n",
    "    list_pieces.append(split)\n",
    "    \n",
    "list_pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The list has 10 elements, one for each transcript\n",
    "len(list_pieces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Another popular text analysis technique is called topic modeling. The ultimate goal of topic modeling is to find various topics that are present in your corpus. Each document in the corpus will be made up of at least one topic, if not multiple topics.\n",
    "\n",
    "In this notebook, we will be covering the steps on how to do **Latent Dirichlet Allocation (LDA)**, which is one of many topic modeling techniques. It was specifically designed for text data.\n",
    "\n",
    "To use a topic modeling technique, you need to provide (1) a document-term matrix and (2) the number of topics you would like the algorithm to pick up.\n",
    "\n",
    "Once the topic modeling technique is applied, your job as a human is to interpret the results and see if the mix of words in each topic make sense. If they don't make sense, you can try changing up the number of topics, the terms in the document-term matrix, model parameters, or even try a different model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's read in our document-term matrix\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "data = pd.read_pickle('dtm_stop.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules for LDA with gensim\n",
    "# Terminal / Anaconda Navigator: conda install -c conda-forge gensim\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One of the required inputs is a term-document matrix\n",
    "tdm = data.transpose()\n",
    "tdm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're going to put the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus\n",
    "sparse_counts = scipy.sparse.csr_matrix(tdm)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim also requires dictionary of the all terms and their respective location in the term-document matrix\n",
    "cv = pickle.load(open(\"cv_stop.pkl\", \"rb\"))\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term), we need to specify two other parameters - the number of topics and the number of passes. Let's start the number of topics at 2, see if the results make sense, and increase the number from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term),\n",
    "# we need to specify two other parameters as well - the number of topics and the number of passes\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA for num_topics = 3\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=3, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA for num_topics = 4\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=4, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These topics aren't looking too great. We've tried modifying our parameters. Let's try modifying our terms list as well.\n",
    "\n",
    "## Topic Modeling - Attempt #2 (Nouns Only)\n",
    "\n",
    "One popular trick is to look only at terms that are from one part of speech (only nouns, only adjectives, etc.). Check out the UPenn tag set: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "def nouns(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    tokenized = word_tokenize(text)\n",
    "    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the cleaned data, before the CountVectorizer step\n",
    "data_clean = pd.read_pickle('data_clean.pkl')\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download()\n",
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "\n",
    "data_nouns = pd.DataFrame(data_clean.transcript.apply(nouns))\n",
    "data_nouns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new document-term matrix using only nouns\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Re-add the additional stop words since we are recreating the document-term matrix\n",
    "add_stop_words = ['like', 'im', 'know', 'just', 'dont', 'thats', 'right', 'people',\n",
    "                  'youre', 'got', 'gonna', 'time', 'think', 'yeah', 'said']\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
    "\n",
    "# Recreate a document-term matrix with only nouns\n",
    "cvn = CountVectorizer(stop_words=stop_words)\n",
    "data_cvn = cvn.fit_transform(data_nouns.transcript)\n",
    "data_dtmn = pd.DataFrame(data_cvn.toarray(), columns=cvn.get_feature_names())\n",
    "data_dtmn.index = data_nouns.index\n",
    "data_dtmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusn = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmn.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordn = dict((v, k) for k, v in cvn.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with 2 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=2, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try topics = 3\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=3, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try 4 topics\n",
    "ldan = models.LdaModel(corpus=corpusn, num_topics=4, id2word=id2wordn, passes=10)\n",
    "ldan.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling - Attempt #3 (Nouns and Adjectives)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function to pull out nouns from a string of text\n",
    "def nouns_adj(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n",
    "    return ' '.join(nouns_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the nouns function to the transcripts to filter only on nouns\n",
    "data_nouns_adj = pd.DataFrame(data_clean.transcript.apply(nouns_adj))\n",
    "data_nouns_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new document-term matrix using only nouns and adjectives, also remove common words with max_df\n",
    "cvna = CountVectorizer(stop_words=stop_words, max_df=.8)\n",
    "data_cvna = cvna.fit_transform(data_nouns_adj.transcript)\n",
    "data_dtmna = pd.DataFrame(data_cvna.toarray(), columns=cvna.get_feature_names())\n",
    "data_dtmna.index = data_nouns_adj.index\n",
    "data_dtmna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the gensim corpus\n",
    "corpusna = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmna.transpose()))\n",
    "\n",
    "# Create the vocabulary dictionary\n",
    "id2wordna = dict((v, k) for k, v in cvna.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with 2 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=2, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try 3 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=3, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try 4 topics\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=10)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Topics in Each Document\n",
    "\n",
    "Out of the 9 topic models we looked at, the nouns and adjectives, 4 topic one made the most sense. So let's pull that down here and run it through some more iterations to get more fine-tuned topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# Our final LDA model (for now)\n",
    "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=80)\n",
    "ldana.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These four topics look pretty decent. Let's settle on these for now.\n",
    "* Topic 0: mom, parents\n",
    "* Topic 1: husband, wife\n",
    "* Topic 2: guns\n",
    "* Topic 3: profanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at which topics each transcript contains\n",
    "corpus_transformed = ldana[corpusna]\n",
    "list(zip([a for [(a,b)] in corpus_transformed], data_dtmna.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a first pass of LDA, these kind of make sense to me, so we'll call it a day for now.\n",
    "* Topic 0: mom, parents [Anthony, Hasan, Louis, Ricky]\n",
    "* Topic 1: husband, wife [Ali, John, Mike]\n",
    "* Topic 2: guns [Bill, Bo, Jim]\n",
    "* Topic 3: profanity [Dave, Joe]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Exercises\n",
    "\n",
    "1. Try further modifying the parameters of the topic models above and see if you can get better topics.\n",
    "2. Create a new topic model that includes terms from a different [part of speech](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) and see if you can get better topics."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
